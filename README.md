# multimedia-feedback-coding

Phase 1 scaffold for a desktop review tool that scans a screenshot project directory,
shows metadata and screenshots, and supports basic navigation.

## Setup (Recommended: uv)

```bash
uv venv
uv sync --extra dev
```

## Run GUI

```bash
uv run python -m screenreview.main
# optional: start with project folder
uv run python -m screenreview.main /path/to/project
```

Alternative entrypoints:

```bash
uv run python -m screenreview.gui
uv run multimedia-feedback-coding-gui
```

## Quality / Tests

```bash
uv run pytest -q
uv run ruff check src tests
uv run ruff format --check src tests
uv run mypy src
```

## OCR Integration

The application includes local OCR processing using EasyOCR for extracting text from screenshots. This provides contextual information for AI analysis without requiring API calls.

### OCR Commands

```bash
# Process OCR on all routes in a project
just ocr-process-routes output/feedback/routes

# Process OCR on a single screenshot
just ocr-process-single output/feedback/routes/login/mobile/screenshot.png

# Extract OCR from a gesture region
just ocr-gesture output/feedback/routes/login/mobile/screenshot.png 195 420

# Show OCR context for AI analysis
just ocr-show output/feedback/routes/login/mobile

# Run complete OCR workflow demo
just ocr-workflow
```

### OCR in AI Analysis

OCR results are automatically integrated into AI analysis prompts:

```
## OCR Text Elements
OCR Text Elements: "Anmelden" at (120, 400), "Email" at (30, 200), "Passwort" at (30, 300)
```

### OCR Data Storage

OCR results are stored in `.extraction/screenshot_ocr.json` files within each viewport directory:

```json
[
  {
    "text": "Anmelden",
    "bbox": {
      "top_left": {"x": 120, "y": 400},
      "bottom_right": {"x": 270, "y": 440}
    },
    "confidence": 0.95
  }
]
```

## Gesture Detection + OCR

The application includes MediaPipe-based gesture detection that can track pointing gestures and combine them with OCR for precise UI feedback.

### Gesture Commands

```bash
# Process gesture annotations with OCR
just ocr-gestures screen_dir gestures.json --transcript-json transcript.json

# Run complete gesture + OCR workflow demo
just gesture-workflow
```

### Gesture Data Format

Gesture events are stored as JSON:

```json
[
  {
    "timestamp": 5.2,
    "frame_index": 156,
    "webcam_position": {"x": 320, "y": 240},
    "screenshot_position": {"x": 195, "y": 420}
  }
]
```

### Gesture Annotations

Gesture annotations combine OCR text with spoken feedback:

```json
[
  {
    "index": 1,
    "timestamp": 5.2,
    "position": {"x": 195, "y": 420},
    "ocr_text": "Anmelden",
    "spoken_text": "Der Anmelden-Button muss entfernt werden",
    "trigger_type": "remove",
    "region_image": "gesture_regions/region_195_420.png"
  }
]
```

### Transcript Integration

Gesture annotations are written to `transcript.md`:

```markdown
## Notes
- [00:05] ðŸ”´ REMOVE: "Der Anmelden-Button muss entfernt werden"
  â†’ Position: (195, 420) | OCR: "Anmelden" | Region: gesture_regions/region_195_420.png
```

## Complete Pipeline (AI Analysis Optional)

The application includes a complete pipeline that processes audio, video, gestures, OCR, and transcripts. **AI analysis is now optional** - you can choose to use expensive AI models or rely on local processing only.

### AI Analysis Toggle

In Settings â†’ AI Analysis tab, you can enable/disable AI analysis:

- **âœ… AI Analysis Enabled**: Uses Replicate/OpenRouter for advanced bug detection
- **âŒ AI Analysis Disabled**: Local processing only (OCR, gestures, transcripts)

When AI is disabled, the system still creates comprehensive bug reports using:
- Trigger word detection from transcripts
- Gesture position analysis
- OCR text extraction
- Local pattern matching

### Pipeline Commands

```bash
# Run complete pipeline (audio + video + OCR + gestures + transcripts)
just complete-pipeline

# Individual components
just ocr-process-routes output/feedback/routes  # OCR processing
just gesture-workflow                          # Gesture + OCR workflow
```

### Pipeline Overview

```
DU VOR BEAMER
     â”‚
     â”œâ”€â†’ Webcam â”€â”€â†’ Video â”€â”€â†’ Frames (FFmpeg, 0â‚¬)
     â”‚                â”‚         â”‚
     â”‚                â”‚         â””â”€â†’ Gestures (MediaPipe, 0â‚¬)
     â”‚                â”‚               â”‚
     â”‚                â”‚               â””â”€â†’ OCR Regions (EasyOCR, 0â‚¬)
     â”‚                â”‚
     â”‚                â””â”€â†’ Smart Select (Python, 0â‚¬)
     â”‚
     â””â”€â†’ Mikrofon â”€â”€â†’ Audio â”€â”€â†’ Transcribe (GPT-4o, ~0.006â‚¬)
                         â”‚
                         â””â”€â†’ Trigger Words (Python, 0â‚¬)

     meta.json â”€â”€â†’ Route, Viewport, Git
     ui-audit.json â”€â”€â†’ Layout Metrics

     ALLES â”€â”€â†’ transcript.md (complete bug report)
```

### Generated Files

After running the complete pipeline, each screen directory contains:

```
.extraction/
â”œâ”€â”€ raw_video.mp4              # Webcam recording
â”œâ”€â”€ raw_audio.wav              # Microphone recording
â”œâ”€â”€ frames/                    # Extracted frames (1/sec)
â”‚   â”œâ”€â”€ frame_0001.png
â”‚   â”œâ”€â”€ frame_0002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ audio_transcription.json   # GPT-4o transcription
â”œâ”€â”€ audio_segments.json        # Processed segments with triggers
â”œâ”€â”€ gesture_events.json        # Detected pointing gestures
â”œâ”€â”€ gesture_regions/           # OCR regions around gestures
â”‚   â”œâ”€â”€ region_001.png
â”‚   â””â”€â”€ region_002.png
â”œâ”€â”€ ocr_results/               # OCR data for regions
â”‚   â”œâ”€â”€ screenshot_ocr.json    # Full screenshot OCR
â”‚   â”œâ”€â”€ region_001_ocr.json
â”‚   â””â”€â”€ region_002_ocr.json
â”œâ”€â”€ gesture_annotations.json   # Combined annotations
â””â”€â”€ trigger_events.json        # Detected trigger words

transcript.md                  # Complete bug report
```

### transcript.md Output

```markdown
# Transcript (Voice -> Text)
Route: /login.html
Viewport: mobile
Size: 390x844
Branch: main
Commit: 8904800cd7d591afb43873fb76cb1fd5272ac957
Timestamp: 2026-02-21T21:43:57Z

## Audio-Transkription
Der Anmelden-Button muss entfernt werden. Das Passwort-Feld soll grÃ¶ÃŸer sein. Der Header passt so.

## Annotationen
- [00:05] ðŸ”´ REMOVE: "Der Anmelden-Button muss entfernt werden"
  â†’ Position: (195, 420) | OCR: "Anmelden"
  â†’ Region: .extraction/gesture_regions/region_001.png

- [00:25] ðŸŸ¡ RESIZE: "Das Passwort-Feld soll grÃ¶ÃŸer sein"
  â†’ Position: (195, 350) | OCR: "Passwort eingeben"
  â†’ Region: .extraction/gesture_regions/region_002.png

## Numbered refs
1: ðŸ”´ REMOVE Anmelden â€“ Der Anmelden-Button muss entfernt werden
2: ðŸŸ¡ RESIZE Passwort eingeben â€“ Das Passwort-Feld soll grÃ¶ÃŸer sein
```

### Cost Breakdown

| Component | Tool | Cost |
|-----------|------|------|
| Video Recording | OpenCV | 0.00â‚¬ |
| Audio Recording | PyAudio | 0.00â‚¬ |
| Frame Extraction | FFmpeg | 0.00â‚¬ |
| Gesture Detection | MediaPipe | 0.00â‚¬ |
| OCR Processing | EasyOCR | 0.00â‚¬ |
| Audio Transcription | GPT-4o | ~0.006â‚¬ |
| Trigger Detection | Python | 0.00â‚¬ |
| Report Generation | Python | 0.00â‚¬ |
| **Total per Screen** | | **~0.006â‚¬** |

**30 screens = ~0.18â‚¬ total** (only transcription costs money).

## Justfile Shortcuts

```bash
just setup
just run
just test
just check
just gui-screenshots
just ocr-process-routes output/feedback/routes
just ocr-workflow
just gesture-workflow
just complete-pipeline
```

## Python Fallback (without uv)

```bash
python3 -m pip install -e .[dev]
python3 -m screenreview.main
```
