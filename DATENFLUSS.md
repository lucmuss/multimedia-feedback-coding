# ScreenReview Datenfluss - Detaillierte Analyse

## Ãœbersicht

Dieses Dokument beschreibt den vollstÃ¤ndigen Datenfluss durch das ScreenReview-System, von der initialen Aufnahme bis zum finalen Bug-Report. Das System ist modular aufgebaut und kann sowohl mit als auch ohne KI-Analyse betrieben werden.

## Architektur-Ãœbersicht

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GUI Layer                         â”‚
â”‚  Settings â”‚ Viewer â”‚ Controls â”‚ Preview â”‚ Progress â”‚ Cost â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚          â”‚          â”‚
      â”‚     User Events     â”‚
      â”‚  (Record/Next/Skip) â”‚
      â”‚          â”‚          â”‚
      â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Queue Manager                         â”‚
â”‚         (ThreadPoolExecutor, max 2 Workers)              â”‚
â”‚                                                         â”‚
â”‚  Screen 1: [B1â†’B2â†’B3â†’B4â†’B5â†’B6â†’B7â†’B8â†’B9]              â”‚
â”‚  Screen 2: [B1â†’B2â†’B3â†’...] (parallel)                   â”‚
â”‚                                                         â”‚
â”‚  Signals â†’ GUI:                                         â”‚
â”‚    progress_updated(screen, step, total, message)        â”‚
â”‚    task_completed(screen, result)                        â”‚
â”‚    cost_updated(total, entry)                           â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                      â”‚
      â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline Layer  â”‚              â”‚    Storage Layer      â”‚
â”‚                  â”‚              â”‚                       â”‚
â”‚  B1: FFmpeg      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  .extraction/frames/  â”‚
â”‚  B2: Smart Selectâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  .extraction/         â”‚
â”‚  B3: MediaPipe   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    gesture_events.jsonâ”‚
â”‚  B4: EasyOCR     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    ocr_results/       â”‚
â”‚  B5: GPT-4o STT  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    audio_*.json       â”‚
â”‚  B6: Triggers    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    trigger_events.jsonâ”‚
â”‚  B7: Korrelation â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    annotations.json   â”‚
â”‚  B8: Analyse*    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚    analysis.json      â”‚
â”‚  B9: Export      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  transcript.md [2]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

* B8 ist optional (Ollama/Cloud oder lokal)
```

## Detaillierter Datenfluss

### Phase 1: Initialisierung & Setup

#### 1.1 Projekt-Setup
```
Input:  Projekt-Verzeichnis (z.B. output/feedback/)
Output: Validierte Projektstruktur

Steps:
â”œâ”€â”€ PrÃ¼fe routes/ Verzeichnis
â”œâ”€â”€ Scanne alle Slugs (login_html/, swipe_overview_html/, ...)
â”œâ”€â”€ Pro Slug prÃ¼fe:
â”‚   â”œâ”€â”€ mobile/ vorhanden?
â”‚   â”œâ”€â”€ desktop/ vorhanden?
â”‚   â”œâ”€â”€ screenshot.png vorhanden?
â”‚   â”œâ”€â”€ meta.json vorhanden und valide? [1]
â”‚   â”‚   â†’ EnthÃ¤lt route, viewport, viewport_size, git, playwright
â”‚   â””â”€â”€ transcript.md vorhanden? [2]
â”‚       â†’ Falls nicht: erstelle Template mit Header aus meta.json [1]
â”œâ”€â”€ Erstelle .extraction/ pro Screen falls nÃ¶tig
â”œâ”€â”€ Lade vorhandene QA-Daten (falls verfÃ¼gbar):
â”‚   â”œâ”€â”€ ui-audit.json â†’ Layout-Metriken, Consistency-Findings [5]
â”‚   â”œâ”€â”€ link-check-report.json â†’ Broken Links [5]
â”‚   â”œâ”€â”€ e2e-report.json â†’ E2E-Status [5]
â”‚   â””â”€â”€ Falls vorhanden: VorausfÃ¼llung der transcript.md [2]
â”‚       mit Auto-Detected Issues aus QA-Daten [5]
â””â”€â”€ Setze Standard-Settings
```

#### 1.2 Settings-Validierung
```
Input:  settings.json + API Keys
Output: Validierte Konfiguration

Components:
â”œâ”€â”€ API Key Validation (OpenAI, Replicate, OpenRouter)
â”œâ”€â”€ Device Detection (Camera, Microphone)
â”œâ”€â”€ Model Availability Check
â””â”€â”€ Settings Persistence
```

### Phase 2: Live-Aufnahme (Recording)

#### 2.1 Screen laden
```
Input:  Aktueller Screen aus Navigator
Output: Screenshot + Metadaten in GUI

Steps:
â”œâ”€â”€ Navigator gibt aktuellen Screen zurÃ¼ck
â”œâ”€â”€ Lade screenshot.png und zeige im Viewer
â”œâ”€â”€ Lade meta.json [1] und zeige Metadaten:
â”‚   Route: /login.html [1]
â”‚   Viewport: mobile [1]
â”‚   Size: 390x844 [1]
â”‚   Branch: main [1]
â”‚   Commit: 8904800... [1]
â”‚   Browser: chromium [1]
â”œâ”€â”€ Falls QA-Daten vorhanden [5]:
â”‚   Zeige Pre-Analysis Score (green/yellow/red)
â”‚   Zeige Auto-Detected Issues
â””â”€â”€ Status: "Bereit fÃ¼r Aufnahme"
```

#### 2.2 Hardware-Initialisierung
```
Input:  Device Indices aus Settings
Output: Initialisierte Capture-Streams

Components:
â”œâ”€â”€ Camera Stream (OpenCV)
â”œâ”€â”€ Audio Stream (PyAudio)
â”œâ”€â”€ Level Monitoring (Background Thread)
â””â”€â”€ Preview Generation (QImage)
```

#### 2.3 Aufnahme starten (Ctrl+R)
```
Input:  Ctrl+R (Start)
Output: Laufende Aufnahme-Threads

Steps:
â”œâ”€â”€ Speicherpfade festlegen:
â”‚   Video: {slug}/{viewport}/.extraction/raw_video.mp4
â”‚   Audio: {slug}/{viewport}/.extraction/raw_audio.wav
â”œâ”€â”€ Video-Thread starten:
â”‚   OpenCV VideoCapture(camera_index)
â”‚   VideoWriter(raw_video.mp4, MP4V, 25fps, 1080p)
â”œâ”€â”€ Audio-Thread starten:
â”‚   PyAudio Stream(rate=16000, channels=1, format=INT16)
â”‚   Schreibe WAV-Header
â”œâ”€â”€ GUI Updates starten:
â”‚   Webcam-Preview (alle 120ms)
â”‚   Audio-Level-Meter (alle 120ms)
â”‚   Aufnahme-Timer (jede 1000ms)
â””â”€â”€ Status: "â— REC 00:00"
```

#### 2.4 Aufnahme stoppen (Ctrl+N oder Ctrl+S)
```
Input:  Ctrl+N (Next) oder Ctrl+S (Stop)
Output: raw_video.mp4 + raw_audio.wav gespeichert

Steps:
â”œâ”€â”€ Video-Thread stoppen â†’ raw_video.mp4 finalisiert
â”œâ”€â”€ Audio-Thread stoppen â†’ raw_audio.wav finalisiert
â”œâ”€â”€ GUI Updates stoppen
â”œâ”€â”€ DateigrÃ¶ÃŸe prÃ¼fen (> 0 Bytes)
â”œâ”€â”€ Task in Queue legen â†’ Phase 3 startet im Hintergrund
â”œâ”€â”€ Falls Ctrl+N: Sofort nÃ¤chsten Screen laden (2.1)
â””â”€â”€ Falls Ctrl+S: Auf aktuellem Screen bleiben
```

### Phase 3: Post-Processing Pipeline
(LÃ¤uft im Hintergrund, GUI nie blockiert)

Reihenfolge und AbhÃ¤ngigkeiten:

B1: Frames extrahieren
    Eingabe:  .extraction/raw_video.mp4
    Tool:     FFmpeg (lokal, 0â‚¬)
    Ausgabe:  .frames/frame_0001.png ... frame_XXXX.png
    AbhÃ¤ngig von: Nichts (erster Schritt)
    Fortschritt: "Frames extrahieren: X von Y"
         â”‚
         â–¼
B2: Smart Selector
    Eingabe:  .frames/frame_*.png + .extraction/raw_audio.wav
    Tools:    MediaPipe + OpenCV + Audio-Level (alle lokal, 0â‚¬)
    Ausgabe:  .frames/selected/frame_*.png
    AbhÃ¤ngig von: B1
    Logik:
    â”œâ”€â”€ Lade alle Frames aus .frames/
    â”œâ”€â”€ FÃ¼r jeden Frame berechne 3 Scores:
    â”‚   â”œâ”€â”€ Gesten-Score: MediaPipe â†’ Hand sichtbar + Zeigefinger?
    â”‚   â”œâ”€â”€ Audio-Score: Audio-Level zum Frame-Zeitpunkt > 0.1?
    â”‚   â””â”€â”€ Diff-Score: Pixel-Differenz zum vorherigen Frame > 5%?
    â”œâ”€â”€ Frame behalten wenn mindestens 1 Score positiv
    â”œâ”€â”€ Immer behalten: erster + letzter Frame
    â”œâ”€â”€ Maximum: max_frames_per_screen aus Settings
    â””â”€â”€ Kopiere ausgewÃ¤hlte Frames nach .frames/selected/
    Fortschritt: "Smart Select: X von Y Frames ausgewÃ¤hlt"
         â”‚
         â–¼
B3: Gesten-Erkennung (detailliert)
    Eingabe:  .frames/selected/frame_*.png
    Tool:     MediaPipe Hands (lokal, 0â‚¬)
    Ausgabe:  .extraction/gesture_events.json
    AbhÃ¤ngig von: B2
    Logik:
    â”œâ”€â”€ FÃ¼r jeden selected Frame:
    â”‚   â”œâ”€â”€ Frame laden (OpenCV BGR â†’ RGB)
    â”‚   â”œâ”€â”€ MediaPipe Hands.process(frame)
    â”‚   â”œâ”€â”€ Falls Hand erkannt:
    â”‚   â”‚   â”œâ”€â”€ PrÃ¼fe Zeigegeste:
    â”‚   â”‚   â”‚   Zeigefinger ausgestreckt (landmark[8].y < landmark[6].y)
    â”‚   â”‚   â”‚   Andere Finger eingeklappt
    â”‚   â”‚   â”œâ”€â”€ Fingerspitze = landmark[8]
    â”‚   â”‚   â”œâ”€â”€ Webcam-Koordinaten (pixel)
    â”‚   â”‚   â”œâ”€â”€ Umrechnung auf Screenshot-Koordinaten:
    â”‚   â”‚   â”‚   Nutze viewport_size aus meta.json [1]
    â”‚   â”‚   â”‚   (390x844 fÃ¼r mobile [1])
    â”‚   â”‚   â””â”€â”€ Speichere Event mit Timestamp
    â”‚   â””â”€â”€ Falls keine Hand: Frame Ã¼berspringen
    â””â”€â”€ Speichere als gesture_events.json
    Fortschritt: "Gesten: X erkannt in Y Frames"
         â”‚
         â–¼
B4: Brush Markings & Intelligent OCR
    Eingabe:  screenshot.png + annotation_overlay.png + gesture_events.json
    Tools:    AnnotationAnalyzer + OcrProcessor (Pillow + EasyOCR)
    Ausgabe:  .extraction/marked_regions/marked_region_*.png
              .extraction/ocr_results/screenshot_ocr.json
    AbhÃ¤ngig von: B3
    Logik:
    â”œâ”€â”€ A) Manuelle Markierungen (Brush):
    â”‚   â”œâ”€â”€ AnnotationAnalyzer erkennt Pixel-Cluster im Overlay
    â”‚   â”œâ”€â”€ Automatischer Zuschnitt (Crops) der markierten Stellen
    â”‚   â””â”€â”€ OCR auf diesen Ausschnitten liefert direkten Kontext
    â”œâ”€â”€ B) Gesten-Regionen:
    â”‚   â”œâ”€â”€ 200x200px Bereich um MediaPipe-Koordinaten ausschneiden
    â”‚   â””â”€â”€ OCR zur Identifikation des fokussierten UI-Elements
    â””â”€â”€ C) Screenshot-OCR (komplett)
    Logik:
    â”œâ”€â”€ A) Screenshot-OCR (komplett):
    â”‚   â”œâ”€â”€ EasyOCR auf screenshot.png
    â”‚   â”œâ”€â”€ Alle erkannten Texte mit Position + Konfidenz
    â”‚   â””â”€â”€ Speichere als screenshot_ocr.json
    â”œâ”€â”€ B) FÃ¼r jede Gesten-Position:
    â”‚   â”œâ”€â”€ 200x200px Bereich aus screenshot.png ausschneiden
    â”‚   â”œâ”€â”€ Region speichern als region_XXX.png
    â”‚   â”œâ”€â”€ EasyOCR auf Region anwenden
    â”‚   â”œâ”€â”€ Koordinaten umrechnen (Region â†’ Screenshot)
    â”‚   â””â”€â”€ Speichere als region_XXX_ocr.json
    â””â”€â”€ Ergebnis: "Anmelden" bei (195, 420) etc.
    Fortschritt: "OCR: X Texte erkannt"
         â”‚
         â–¼
B5: Audio transkribieren                    â† EINZIGER CLOUD-SCHRITT
    Eingabe:  .extraction/raw_audio.wav
    Tool:     OpenAI GPT-4o Transcribe (Cloud, ~0,6 Cent/Min)
    Ausgabe:  .extraction/audio_transcription.json
              .extraction/audio_transcription.txt
              .extraction/audio_segments.json
    AbhÃ¤ngig von: Nichts (kann parallel zu B1-B4 laufen!)
    Logik:
    â”œâ”€â”€ Audio an OpenAI API senden
    â”œâ”€â”€ Sprache: Deutsch (aus Settings)
    â”œâ”€â”€ Format: verbose_json mit Segment-Timestamps
    â”œâ”€â”€ Ergebnis parsen:
    â”‚   â”œâ”€â”€ audio_transcription.json (komplett mit Metadaten)
    â”‚   â”œâ”€â”€ audio_transcription.txt (nur Volltext)
    â”‚   â””â”€â”€ audio_segments.json (nur Segmente mit Timestamps)
    â””â”€â”€ Kosten tracken â†’ Cost Widget aktualisieren
    Fortschritt: "Transkription: abgeschlossen (0,006â‚¬)"
         â”‚
         â–¼
B6: Trigger-WÃ¶rter erkennen
    Eingabe:  .extraction/audio_segments.json
    Tool:     Python String-Matching (lokal, 0â‚¬)
    Ausgabe:  .extraction/trigger_events.json
    AbhÃ¤ngig von: B5
    Logik:
    â”œâ”€â”€ FÃ¼r jedes Segment:
    â”‚   â”œâ”€â”€ Text in Kleinbuchstaben
    â”‚   â”œâ”€â”€ Suche nach Trigger-WÃ¶rtern (aus Settings):
    â”‚   â”‚   â”œâ”€â”€ "bug","fehler","falsch"      â†’ bug ðŸ”´
    â”‚   â”‚   â”œâ”€â”€ "ok","passt","gut"           â†’ ok âœ…
    â”‚   â”‚   â”œâ”€â”€ "entfernen","weg","lÃ¶schen"  â†’ remove ðŸ”´
    â”‚   â”‚   â”œâ”€â”€ "grÃ¶ÃŸer","kleiner"           â†’ resize ðŸŸ¡
    â”‚   â”‚   â”œâ”€â”€ "verschieben","bewegen"      â†’ move ðŸŸ¡
    â”‚   â”‚   â”œâ”€â”€ "farbe","style"              â†’ restyle ðŸŸ¡
    â”‚   â”‚   â””â”€â”€ "wichtig","dringend"         â†’ high_priority ðŸ”´
    â”‚   â”œâ”€â”€ Mehrere Trigger pro Segment mÃ¶glich
    â”‚   â””â”€â”€ PrimÃ¤rer Trigger = erster gefundener (nach PrioritÃ¤t)
    â””â”€â”€ Speichere als trigger_events.json
    Fortschritt: "Trigger: X erkannt"
         â”‚
         â–¼
B7: Korrelation (Alles zusammenfÃ¼hren)
    Eingabe:  gesture_events.json
              + audio_segments.json
              + trigger_events.json
              + ocr_results/region_*_ocr.json
    Tool:     Python (lokal, 0â‚¬)
    Ausgabe:  .extraction/annotations.json
    AbhÃ¤ngig von: B3 + B4 + B5 + B6 (alle mÃ¼ssen fertig sein)
    Logik:
    â”œâ”€â”€ FÃ¼r jede Geste:
    â”‚   â”œâ”€â”€ Finde Audio-Segment wo:
    â”‚   â”‚   segment.start <= geste.timestamp <= segment.end
    â”‚   â”‚   (Toleranz: Â±2 Sekunden)
    â”‚   â”œâ”€â”€ Finde Trigger fÃ¼r dieses Segment
    â”‚   â”œâ”€â”€ Finde OCR-Text fÃ¼r diese Position
    â”‚   â””â”€â”€ Erstelle Annotation:
    â”‚       {
    â”‚         index, timestamp,
    â”‚         position: {x, y},
    â”‚         ocr_text: "Anmelden",
    â”‚         spoken_text: "Der Button muss weg",
    â”‚         trigger_type: "remove",
    â”‚         region_image: "gesture_regions/region_001.png"
    â”‚       }
    â”œâ”€â”€ Falls Segmente OHNE Geste:
    â”‚   â””â”€â”€ Trotzdem als Annotation aufnehmen
    â”‚       (Position: null, nur Text + Trigger)
    â””â”€â”€ Sortiere nach Timestamp
    Fortschritt: "Korrelation: X Annotationen erstellt"
         â”‚
         â–¼
B8: KI-Analyse (OPTIONAL)
    Eingabe:  annotations.json + screenshot.png + selected frames
    Tool:     Ollama lokal (0â‚¬) ODER Cloud (Replicate/OpenRouter)
    Ausgabe:  .extraction/analysis.json
    AbhÃ¤ngig von: B7
    â”œâ”€â”€ Falls deaktiviert: Ãœberspringe â†’ weiter zu B9
    â”œâ”€â”€ Falls Ollama:
    â”‚   â”œâ”€â”€ Prompt bauen mit meta.json [1] Daten
    â”‚   â”œâ”€â”€ Bilder als Base64 kodieren
    â”‚   â”œâ”€â”€ POST http://localhost:11434/api/generate
    â”‚   â””â”€â”€ Kosten: 0,00â‚¬
    â””â”€â”€ Falls Cloud:
        â”œâ”€â”€ Prompt + Bilder an API senden
        â””â”€â”€ Kosten tracken
    Fortschritt: "Analyse: abgeschlossen"
         â”‚
         â–¼
B9: transcript.md schreiben
    Eingabe:  annotations.json + meta.json [1] + audio_transcription.txt
              + screenshot_ocr.json + analysis.json (optional)
    Tool:     Python (lokal, 0â‚¬)
    Ausgabe:  transcript.md [2] (befÃ¼llt)
    AbhÃ¤ngig von: B7 (oder B8 falls KI aktiv)
    Logik:
    â”œâ”€â”€ Header aus meta.json [1]:
    â”‚   Route, Viewport, Size, Browser, Branch, Commit, Timestamp
    â”œâ”€â”€ Audio-Transkription (Volltext)
    â”œâ”€â”€ Annotationen mit Icons:
    â”‚   ðŸ”´ fÃ¼r bug/remove/high_priority
    â”‚   ðŸŸ¡ fÃ¼r resize/move/restyle
    â”‚   âœ… fÃ¼r ok
    â”‚   ðŸ“ fÃ¼r unklassifiziert
    â”œâ”€â”€ Screenshot OCR (alle Texte)
    â”œâ”€â”€ Falls KI-Analyse vorhanden:
    â”‚   â””â”€â”€ KI-generierte Bug-Zusammenfassung
    â”œâ”€â”€ Numbered Refs (priorisierte Liste)
    â””â”€â”€ Schreibe transcript.md [2]
    Fortschritt: "Export: transcript.md geschrieben âœ…"

Parallelisierung innerhalb eines Screens:

Thread 1 (Video-Pipeline):     Thread 2 (Audio-Pipeline):
B1: Frames extrahieren          B5: Audio transkribieren
         â”‚                               â”‚
         â–¼                               â–¼
B2: Smart Selector              B6: Trigger-WÃ¶rter
         â”‚                               â”‚
         â–¼                               â”‚
B3: Gesten-Erkennung                     â”‚
         â”‚                               â”‚
         â–¼                               â”‚
B4: OCR                                  â”‚
         â”‚                               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              B7: Korrelation
                     â”‚
                     â–¼
              B8: KI-Analyse (optional)
                     â”‚
                     â–¼
              B9: Export transcript.md [2]

Zeitersparnis: ~30-40% weil Audio und Video
parallel verarbeitet werden.

### Phase 4: Annotations & Korrelation

#### 4.1 Zeitbasierte Korrelation
```
Input:  gesture_events.json + audio_segments.json
Output: gesture_annotations.json

Algorithm:
â”œâ”€â”€ For each gesture event:
â”‚   â”œâ”€â”€ Find closest audio segment by timestamp
â”‚   â”œâ”€â”€ Match within Â±2 second tolerance
â”‚   â”œâ”€â”€ Extract OCR text from gesture region
â”‚   â”œâ”€â”€ Apply trigger classification
â”‚   â””â”€â”€ Create annotation record
â””â”€â”€ Sort by timestamp
```

#### 4.2 Lokale Analyse (Fallback)
```
Input:  transcript_segments + gesture_positions
Output: Local bug reports

Algorithm:
â”œâ”€â”€ Simple keyword matching (bug, fehler, remove, etc.)
â”œâ”€â”€ Gesture position analysis
â”œâ”€â”€ Priority assignment (high/medium/low)
â””â”€â”€ Basic issue categorization
```

### Phase 5: KI-Analyse (Optional)

#### 5.1 Prompt-Generierung
```
Input:  extraction_result + settings
Output: Formatted prompt for AI model

Components:
â”œâ”€â”€ Screenshot + Gesture Regions
â”œâ”€â”€ OCR Context Integration
â”œâ”€â”€ Transcript Segments
â”œâ”€â”€ Meta Data (route, viewport, git)
â””â”€â”€ Instruction Template
```

#### 5.2 AI-Model-Aufruf
```
Input:  Images + Prompt + API Key
Output: Raw AI response

Providers:
â”œâ”€â”€ Replicate (llama_32_vision, qwen_vl)
â”œâ”€â”€ OpenRouter (gpt4o_vision, llama_32_vision, qwen_vl)
â””â”€â”€ Fallback: Local analysis
```

#### 5.3 Response-Parsing
```
Input:  Raw AI response (JSON/markdown)
Output: Structured bug reports

Algorithm:
â”œâ”€â”€ JSON Detection & Parsing
â”œâ”€â”€ Markdown Table Parsing
â”œâ”€â”€ Issue Normalization (id, element, position, etc.)
â”œâ”€â”€ Priority Mapping
â””â”€â”€ Validation & Defaults
```

### Phase 6: Report-Generierung

#### 6.1 transcript.md Assembly
```
Input:  annotations.json + meta.json + transcript.json
Output: transcript.md (final bug report)

Structure:
â”œâ”€â”€ Header (Route, Viewport, Size, Git, Timestamp)
â”œâ”€â”€ Audio-Transkription (full text)
â”œâ”€â”€ Annotationen (zeitgesteuert mit OCR)
â”œâ”€â”€ Numbered Refs (priorisierte Liste)
â””â”€â”€ Footer (OCR summary, metadata)
```

#### 6.2 Export-Formate
```
Input:  analysis_result
Output: markdown/json exports

Formats:
â”œâ”€â”€ Markdown (human readable)
â”œâ”€â”€ JSON (machine readable)
â””â”€â”€ Auto-export after analysis (configurable)
```

## Zentrale Ordnerstruktur

```
{slug}/
â””â”€â”€ {viewport}/                          (mobile oder desktop)
    â”œâ”€â”€ screenshot.png                   (von capture [3])
    â”œâ”€â”€ meta.json                        [1]
    â”œâ”€â”€ transcript.md                    [2]
    â”‚
    â”œâ”€â”€ .frames/                         (von ScreenReview AI)
    â”‚   â”œâ”€â”€ frame_0001.png              â† B1
    â”‚   â”œâ”€â”€ frame_0002.png
    â”‚   â”œâ”€â”€ ...
    â”‚   â””â”€â”€ selected/                   â† B2
    â”‚       â”œâ”€â”€ frame_0003.png
    â”‚       â””â”€â”€ ...
    â”‚
    â””â”€â”€ .extraction/                     (von ScreenReview AI)
        â”œâ”€â”€ raw_video.mp4               â† Phase 2 (Aufnahme)
        â”œâ”€â”€ raw_audio.wav               â† Phase 2 (Aufnahme)
        â”‚
        â”œâ”€â”€ frames/                     â† B1 (Frame-Extraktion)
        â”‚   â”œâ”€â”€ frame_0001.png
        â”‚   â”œâ”€â”€ frame_0002.png
        â”‚   â””â”€â”€ ...
        â”‚
        â”œâ”€â”€ audio_transcription.json    â† B5 (komplett)
        â”œâ”€â”€ audio_transcription.txt     â† B5 (nur Text)
        â”œâ”€â”€ audio_segments.json         â† B5 (mit Timestamps)
        â”‚
        â”œâ”€â”€ trigger_events.json         â† B6
        â”œâ”€â”€ gesture_events.json         â† B3
        â”‚
        â”œâ”€â”€ gesture_regions/            â† B4
        â”‚   â”œâ”€â”€ region_001.png
        â”‚   â””â”€â”€ ...
        â”‚
        â”œâ”€â”€ ocr_results/               â† B4
        â”‚   â”œâ”€â”€ screenshot_ocr.json
        â”‚   â”œâ”€â”€ region_001_ocr.json
        â”‚   â””â”€â”€ ...
        â”‚
        â”œâ”€â”€ annotations.json           â† B7 (alles zusammen)
        â”œâ”€â”€ analysis.json              â† B8 (optional, KI)
        â””â”€â”€ debug.log                  â† Logging
```

## Datenformate & Schnittstellen

### Core Data Structures

#### ExtractionResult
```python
@dataclass
class ExtractionResult:
    screen: ScreenInfo
    selected_frames: List[Path]
    gesture_positions: List[Dict[str, Any]]
    transcript_segments: List[Dict[str, Any]]
    ocr_results: Dict[str, Any]
```

#### AnalysisResult
```python
@dataclass
class AnalysisResult:
    screen: ScreenInfo
    bugs: List[Dict[str, Any]]
    summary: str
    raw_response: str
    model_used: str
    cost_euro: float
```

### JSON Schema Examples

#### gesture_events.json
```json
[
  {
    "timestamp": 10.5,
    "frame_index": 3,
    "webcam_position": {"x": 320, "y": 240},
    "screenshot_position": {"x": 195, "y": 420}
  }
]
```

#### audio_segments.json
```json
[
  {
    "start": 8.0,
    "end": 12.0,
    "text": "Der Button muss entfernt werden",
    "triggers": [
      {"type": "remove", "word": "entfernt", "text": "..."}
    ],
    "primary_trigger": "remove"
  }
]
```

#### gesture_annotations.json
```json
[
  {
    "index": 1,
    "timestamp": 10.5,
    "position": {"x": 195, "y": 420},
    "ocr_text": "Anmelden",
    "spoken_text": "Der Button muss entfernt werden",
    "trigger_type": "remove",
    "region_image": "gesture_regions/region_001.png"
  }
]
```

## Fehlerbehandlung & Fallbacks

### Component-Level Fallbacks

#### MediaPipe nicht verfÃ¼gbar
```
â†’ GestureDetector.__init__() graceful degradation
â†’ Logging warning, continue without gesture detection
â†’ Smart Selector relies only on audio levels
```

#### FFmpeg nicht verfÃ¼gbar
```
â†’ FrameExtractor.extract_frames() returns empty list
â†’ Logging error, skip frame-based analysis
â†’ Continue with screenshot-only OCR
```

#### EasyOCR nicht verfÃ¼gbar
```
â†’ OcrProcessor uses fallback text extraction
â†’ Empty OCR results, continue with gesture-only
â†’ transcript.md notes missing OCR
```

#### API Keys fehlen
```
â†’ Analyzer._create_local_analysis_result()
â†’ Local trigger-based analysis
â†’ Cost = 0.0, model_used = "local"
```

#### B5 GPT-4o Transcribe fehlschlÃ¤gt
```
â”œâ”€â”€ Fallback 1: Whisper lokal (falls installiert)
â”‚   pip install openai-whisper
â”‚   whisper raw_audio.wav --model small --language de
â”œâ”€â”€ Fallback 2: Whisper auf Replicate
â”‚   (falls Replicate Key vorhanden)
â”œâ”€â”€ Fallback 3: Ohne Transkription fortfahren
â”‚   â†’ audio_transcription.txt = "(Transkription fehlgeschlagen)"
â”‚   â†’ annotations.json nur mit Gesten + OCR (kein gesprochener Text)
â”‚   â†’ transcript.md [2] enthÃ¤lt Hinweis:
â”‚     "âš ï¸ Audio-Transkription fehlgeschlagen â€“ nur OCR-Daten"
```

#### Beamer-Region nicht kalibriert
```
â”œâ”€â”€ Fallback: Verwende gesamten Frame als Beamer-Bereich
â”œâ”€â”€ Gesten-Koordinaten sind ungenauer aber nutzbar
â””â”€â”€ Hinweis in transcript.md [2]:
    "âš ï¸ Beamer-Region nicht kalibriert â€“ Positionen approximiert"
```

#### Keine Gesten erkannt (z.B. Stock nicht sichtbar)
```
â”œâ”€â”€ Alle Frames werden trotzdem behalten
â”œâ”€â”€ OCR lÃ¤uft auf gesamtem Screenshot
â”œâ”€â”€ Transkript wird ohne Positionsangaben geschrieben
â””â”€â”€ transcript.md [2] enthÃ¤lt nur Audio-Text ohne Position
```

### Pipeline-Level Resilience

#### Einzelne Componenten fehlschlagen
```
â†’ Continue with remaining components
â†’ Partial results in transcript.md
â†’ Clear indication of missing data
```

#### VollstÃ¤ndiger Pipeline-Abbruch
```
â†’ Save intermediate results
â†’ Generate partial transcript.md
â†’ User notification with recovery options
```

## Performance & Ressourcen

### Memory Usage
```
Base Application: ~50MB
With MediaPipe: +200MB
With EasyOCR: +500MB
With Large Images: +100MB per screenshot
Peak: ~1GB during full pipeline
```

### CPU/GPU Usage
```
Frame Extraction: CPU intensive (FFmpeg)
Gesture Detection: CPU/GPU (MediaPipe)
OCR Processing: CPU/GPU (EasyOCR)
AI Analysis: GPU preferred (Replicate/OpenRouter)
Audio Processing: CPU (Whisper local)
```

### Network Usage
```
Audio Transcription: ~0.006â‚¬ per minute
AI Analysis: 0.01-0.02â‚¬ per screenshot
Total per 30min session: ~0.18-0.60â‚¬
```

## Monitoring & Debugging

### Log Levels
```
DEBUG: Detailed component operations
INFO: Major pipeline steps completion
WARNING: Missing dependencies, API failures
ERROR: Component failures, data corruption
```

### Debug Output
```
.extraction/debug.log
â”œâ”€â”€ Timestamped operations
â”œâ”€â”€ Performance metrics
â”œâ”€â”€ API call details (sanitized)
â””â”€â”€ Error stack traces
```

### Health Checks
```
API Connectivity: Settings dialog validation
Device Availability: Test Webcam/Audio buttons
Model Access: Test Models button
Pipeline Integrity: Preflight check
```

## Erweiterbarkeit

### Neue AI Provider
```
1. Implement Client class (integrations/new_provider_client.py)
2. Add to Analyzer.__init__()
3. Update SettingsDialog._build_analysis_tab()
4. Add MODEL_PRICE_EURO entry
5. Test with _compute_api_validation_result()
```

### Neue Trigger-Kategorien
```
1. Add to TriggerDetector.TRIGGER_WORDS
2. Update PRIORITY_ORDER if needed
3. Add icon mapping in transcript generation
4. Test with sample transcripts
```

### Neue Export-Formate
```
1. Implement exporter (pipeline/new_exporter.py)
2. Add to SettingsDialog._build_export_tab()
3. Update Exporter class
4. Add file extension handling
```

## Testing & Debugging der Pipeline (Anweisungen fÃ¼r AI-Agenten)

FÃ¼r die kontinuierliche Entwicklung und Fehlersuche stehen spezifische Skripte und Methoden bereit:

### 1. Komponenten-Check (`test_pipeline_check.py`)
Dieses Skript validiert, ob alle Basis-Komponenten und deren AbhÃ¤ngigkeiten erfolgreich importiert werden kÃ¶nnen. Es sollte nach jeder Ã„nderung an den Dependencies oder Core-Klassen (z. B. `GestureDetector`, `OCRProcessor`) ausgefÃ¼hrt werden:
```bash
uv run python3 test_pipeline_check.py
```

### 2. Pipeline-Dry-Run auf Realdaten (`scripts/debug_pipeline.py`)
Dieses Skript ist ideal, um die einzelnen Pipeline-Schritte (Frame Extraction, Gestenerkennung, OCR) isoliert und ohne Cloud-Kosten auf **echten Aufnahme-Daten** zu testen.

*   **Voraussetzung:** Es muss ein gÃ¼ltiger Extraktionsordner vorhanden sein (mit `raw_video.avi` und `raw_audio.wav`).
*   **Wichtiger Tipp fÃ¼r PaddleOCR:** Um lange Wartezeiten durch Modell-Quellen-Checks (insbesondere ohne Netzwerkverbindung oder in CI/CD) zu vermeiden, nutze das Flag:
    ```bash
    export PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK=True
    uv run python3 scripts/debug_pipeline.py
    ```
*   **OCR-QualitÃ¤t:** Beachte, dass die OCR auf den verkleinerten Video-Frames oft schlechtere Ergebnisse liefert als auf dem finalen High-Res `screenshot.png` der Route. Um OCR explizit auf QualitÃ¤t zu testen, wende die `OCRProcessor.process()`-Methode direkt auf den `screenshot.png` an.

### 3. Spezifische Backend-Anpassungen (Hardware/Treiber)
*   **GoPro/UDP-Streams:** Wenn VideoCapture fÃ¼r Netzwerk-Streams verwendet wird, ist das `cv2.CAP_FFMPEG`-Backend zwingend zu bevorzugen. Parameter wie `?overrun_nonfatal=1&fifo_size=50000000` verhindern Latenzen.
*   **Windows Camera Exceptions:** Hardware-Kameras kÃ¶nnen bei `cv2.VideoCapture.set()`-Aufrufen (z. B. fÃ¼r Framerate oder AuflÃ¶sung) hart crashen (`Unknown C++ exception`). Diese Aufrufe mÃ¼ssen in der Pipeline immer in defensive `try...except`-BlÃ¶cke gewrappt sein.
*   **MediaPipe Legacy vs. Tasks:** Die Pipeline verwendet in Version `0.10.x` die alte `solutions`-API. Auf einigen System-Plattformen ist dieser Namespace defekt. Der `GestureDetector` ist so gebaut, dass er diesen Ausfall "graceful" behandelt und einfach keine Gesten liefert, anstatt die gesamte Pipeline zum Stillstand zu bringen.

Dieses Dokument beschreibt den vollstÃ¤ndigen Datenfluss durch ScreenReview. Das System ist robust designed mit umfassenden Fallback-Mechanismen und kann sowohl vollstÃ¤ndig lokal als auch mit Cloud-Komponenten betrieben werden.
